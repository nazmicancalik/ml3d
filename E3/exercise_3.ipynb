{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 3: Shape Reconstruction\n",
    "\n",
    "**Submission Deadline**: 09.06.2021, 23:55\n",
    "\n",
    "We will take a look at two major approaches for 3D shape reconstruction in this last exercise.\n",
    "\n",
    "Like in exercise 2, you can run all trainings either locally or on Google Colab. Just follow the instructions below. \n",
    "\n",
    "Note that training reconstruction methods generally takes relatively long, even for simple shape completion. Training the generalization will take a few hours. *Thus, please make sure to start training well before the submission deadline.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Running this notebook\n",
    "We recommend running this notebook on a cuda compatible local gpu. You can also run training on cpu, it will just take longer.\n",
    "\n",
    "We describe two options for executing the training parts of this exercise below: Using Google Colab or running it locally on your machine. If you are not planning on using Colab, just skip forward to Local Execution.\n",
    "\n",
    "### Google Colab\n",
    "\n",
    "If you don't have access to gpu and don't wish to train on CPU, you can use Google Colab. However, we experienced the issue that inline visualization of shapes or inline images didn't work on colab, so just keep that in mind.\n",
    "What you can also do is only train networks on colab, download the checkpoint, and visualize inference locally.\n",
    "\n",
    "In case you're using Google Colab, you can upload the exercise folder (containing `exercise_3.ipynb`, directory `exercise_3` and the file `requirements.txt`) as `3d-machine-learning` to google drive (make sure you don't upload extracted datasets files).\n",
    "Additionally you'd need to open the notebook `exercise_3.ipynb` in Colab using `File > Open Notebook > Upload`.\n",
    "\n",
    "Next you'll need to run these two cells for setting up the environment. Before you do that make sure your instance has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\nfrom google.colab import drive\\ndrive.mount('/content/drive', force_remount=True)\\n\\n# We assume you uploaded the exercise folder in root Google Drive folder\\n\\n!cp -r /content/drive/MyDrive/3d-machine-learning 3d-machine-learning/\\nos.chdir('/content/3d-machine-learning/')\\nprint('Installing requirements')\\n!pip install -r requirements.txt\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# We assume you uploaded the exercise folder in root Google Drive folder\n",
    "\n",
    "!cp -r /content/drive/MyDrive/3d-machine-learning 3d-machine-learning/\n",
    "os.chdir('/content/3d-machine-learning/')\n",
    "print('Installing requirements')\n",
    "!pip install -r requirements.txt\n",
    "'''\n",
    "# Make sure you restart runtime when directed by Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell after restarting your colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport sys\\nimport torch\\nos.chdir(\\'/content/3d-machine-learning/\\')\\nsys.path.insert(1, \"/content/3d-machine-learning/\")\\nprint(\\'CUDA availability:\\', torch.cuda.is_available())\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "os.chdir('/content/3d-machine-learning/')\n",
    "sys.path.insert(1, \"/content/3d-machine-learning/\")\n",
    "print('CUDA availability:', torch.cuda.is_available())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Execution\n",
    "\n",
    "If you run this notebook locally, you have to first install the python dependiencies again. They are the same as for exercise 1 so you can re-use the environment you used last time. If you use [poetry](https://python-poetry.org), you can also simply re-install everything (`poetry install`) and then run this notebook via `poetry run jupyter notebook`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following imports should work regardless of whether you are using Colab or local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to test whether a GPU was detected by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Shape Reconstruction from 3D SDF grids with 3D-EPN\n",
    "\n",
    "In the first part of this exercise, we will take a look at shape complation using [3D-EPN](https://arxiv.org/abs/1612.00101). This approach was also introduced in the lecture.\n",
    "\n",
    "The visualization below shows an overview of the method: From an incomplete shape observation (which you would get when scanning an object with a depth sensor for example), we use a 3D encoder-predictor network that first encodes the incomplete shapes into a common latent space using several 3D convolution layers and then decodes them again using multiple 3D transpose convolutions.\n",
    "\n",
    "This way, we get from a 32^3 SDF voxel grid to a 32^3 DF (unsigned) voxel grid that represents the completed shape. We only focus on this part here; in the original implementation, this 32^3 completed prediction would then be further improved (in an offline step after inference) by sampling parts from a shape database to get the final resolution to 128^3.\n",
    "\n",
    "<img src=\"exercise_3/images/3depn_teaser.png\" alt=\"3D-EPN Teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "The next steps will follow the structure we established in exercise 2: Taking a look at the dataset structure and downloading the data; then, implementing dataset, model, and training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Downloading the data\n",
    "We will use the original dataset used in the official implementation. It consists of SDF and DF grids (representing incomplete input data and complete target data) with a resolution of 32^3 each. Each input-target pair is generated from a ShapeNet shape.\n",
    "\n",
    "The incomplete SDF data are generated by sampling virtual camera trajectories around every object. Each trajectory is assigned an ID which is part of the file names (see below). The camera views for each trajectory are combined into a common SDF grid by volumetric fusion. It is easy to generate an SDF here since we know both camera location and object surface: Everything between camera and surface is known free space and outside the object, leading to a positive SDF sign. Everything behind the surface has a negative sign. For the complete shapes, however, deciding whether a voxel in the DF grid is inside or outside an object is not a trivial problem. This is why we use unsigned distance fields as target and prediction representation instead. This still encodes the distance to the closest surface but does not contain explicit information about the inside/outside location.\n",
    "\n",
    "In terms of dataset layout, we follow the ShapeNet directory structure as seen in the last exercise:\n",
    "Each folder in the `exercise_3/data/shapenet_dim32_sdf` and `exercise_3/data/shapenet_dim32_df` directories contains one shape category represented by a number, e.g. `02691156`.\n",
    "We provide the mapping between these numbers and the corresponding names in `exercise_3/data/shape_info.json`. Each of these shape category folders contains lots of shapes in sdf or df format. In addition to that, every shape now also contains multiple trajectories: 0 to 7, encoded as `__0__` to `__7__`. These 8 files are just different input representations, meaning they vary in the level of completeness and location of missing parts; they all map to the `.df` file with corresponding shape ID and `__0__` at the end.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/shapenet_dim32_sdf\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.sdf   # Trajectory 0 for a shape of the category\n",
    "    ├── 10155655850468db78d106ce0a280f87__1__.sdf   # Trajectory 1 for the same shape\n",
    "    ├── :                                      \n",
    "    ├── 10155655850468db78d106ce0a280f87__7__.sdf   # Trajectory 7 for the same shape\n",
    "    ├── 1021a0914a7207aff927ed529ad90a11__0__.sdf   # Trajectory 0 for another shape\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 8 shape category folders\n",
    ":\n",
    "\n",
    "# contents of exercise_2/data/shapenet_dim32_df\n",
    "02691156/                                           # Shape category folder with all its shapes\n",
    "    ├── 10155655850468db78d106ce0a280f87__0__.df    # A single shape of the category\n",
    "    ├── 1021a0914a7207aff927ed529ad90a11__0__.df    # Another shape of the category\n",
    "    ├── :                                           # And so on ...\n",
    "02933112/                                           # Another shape category folder\n",
    "02958343/                                           # In total you should have 55 shape category folders\n",
    ":\n",
    "```\n",
    "\n",
    "Download and extract the data with the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Downloading ...')\\n# File sizes: 11GB for shapenet_dim32_sdf.zip (incomplete scans), 4GB for shapenet_dim32_df.zip (target shapes)\\n!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_sdf.zip -P ~/datasets/e3\\n!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_df.zip -P ~/datasets/e3\\nprint('Extracting ...')\\n!unzip -q ~/datasets/e3/shapenet_dim32_sdf.zip -d ~/datasets/e3\\n!unzip -q ~/datasets/e3/shapenet_dim32_df.zip -d ~/datasets/e3\\n!rm ~/datasets/e3/shapenet_dim32_sdf.zip\\n!rm ~/datasets/e3/shapenet_dim32_df.zip\\nprint('Done.')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('Downloading ...')\n",
    "# File sizes: 11GB for shapenet_dim32_sdf.zip (incomplete scans), 4GB for shapenet_dim32_df.zip (target shapes)\n",
    "!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_sdf.zip -P ~/datasets/e3\n",
    "!wget http://kaldir.vc.in.tum.de/adai/CNNComplete/shapenet_dim32_df.zip -P ~/datasets/e3\n",
    "print('Extracting ...')\n",
    "!unzip -q ~/datasets/e3/shapenet_dim32_sdf.zip -d ~/datasets/e3\n",
    "!unzip -q ~/datasets/e3/shapenet_dim32_df.zip -d ~/datasets/e3\n",
    "!rm ~/datasets/e3/shapenet_dim32_sdf.zip\n",
    "!rm ~/datasets/e3/shapenet_dim32_df.zip\n",
    "print('Done.')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "The dataset implementation follows the same general structure as in exercise 2. We prepared an initial implementation already in `exercise_3/data/shapenet.py`; your task is to resolve all TODOs there.\n",
    "\n",
    "The data for SDFs and DFs in `.sdf`/`.df` files are stored in binary form as follows:\n",
    "```\n",
    "dimX    #uint64 \n",
    "dimY    #uint64 \n",
    "dimZ    #uint64 \n",
    "data    #(dimX*dimY*dimZ) floats for sdf/df values\n",
    "```\n",
    "The SDF values stored per-voxel represent the distance to the closest surface *in voxels*.\n",
    "\n",
    "You have to take care of three important steps before returning the SDF and DF for the corresponding `index` in `__getitem__`:\n",
    "1. **Truncation**: 3D-EPN uses a truncated SDF which means that for each voxel, the distance to the closest surface will be clamped to a max absolute value. This is helpful since we do not care about longer distances (Marching Cubes only cares about distances close to the surface). It allows us to focus our predictions on the voxels near the surface. We use a `truncation_distance` of 3 (voxels) which means we expect to get an SDF with values between -3 and 3 as input to the model.\n",
    "2. **Separation** of distances and sign: 3D-EPN uses as input a 2x32x32x32 SDF grid, with absolute distance values of the SDF in channel 0 and the signs (-1 or 1) in channel 1.\n",
    "3. **Log** scaling: We scale targets and prediction with a log operation to further guide predictions to focus on the surface voxels. Therefore, you should return target DFs as `log(df + 1)`.\n",
    "\n",
    "**Hint**: An easy way to load the data from `.sdf` and `.df` files is to use `np.fromfile`. First, load the dimensions, then the data, then reshape everything into the shape you loaded in the beginning. Make sure you get the datatypes and byte offsets right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 153540\n",
      "Length of val set: 32304\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train')\n",
    "val_dataset = ShapeNet('val')\n",
    "overfit_dataset = ShapeNet('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 32304\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
      "Input SDF: (2, 32, 32, 32)\n",
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104f58a86d2a489f9602a0ffddf17798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some shapes\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "train_sample = train_dataset[1]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03001627/798a46965d9e0edfcea003eff0268278__3__-03001627/798a46965d9e0edfcea003eff0268278__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_sample = train_dataset[223]\\nprint(f\\'Name: {train_sample[\"name\"]}\\')  # expected output: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\\nprint(f\\'Input SDF: {train_sample[\"input_sdf\"].shape}\\')  # expected output: (2, 32, 32, 32)\\nprint(f\\'Target DF: {train_sample[\"target_df\"].shape}\\')  # expected output: (32, 32, 32)\\n\\ninput_mesh = marching_cubes(train_sample[\\'input_sdf\\'][0], level=1)\\nvisualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_sample = train_dataset[223]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 04379243/a1be21c9a71d133dc5beea20858a99d5__5__-04379243/a1be21c9a71d133dc5beea20858a99d5__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_sample = train_dataset[95]\\nprint(f\\'Name: {train_sample[\"name\"]}\\')  # expected output: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\\nprint(f\\'Input SDF: {train_sample[\"input_sdf\"].shape}\\')  # expected output: (2, 32, 32, 32)\\nprint(f\\'Target DF: {train_sample[\"target_df\"].shape}\\')  # expected output: (32, 32, 32)\\n\\ninput_mesh = marching_cubes(train_sample[\\'input_sdf\\'][0], level=1)\\nvisualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_sample = train_dataset[95]\n",
    "print(f'Name: {train_sample[\"name\"]}')  # expected output: 03636649/3889631e42a84b0f51f77a6d7299806__2__-03636649/3889631e42a84b0f51f77a6d7299806__0__\n",
    "print(f'Input SDF: {train_sample[\"input_sdf\"].shape}')  # expected output: (2, 32, 32, 32)\n",
    "print(f'Target DF: {train_sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(train_sample['input_sdf'][0], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Model\n",
    "\n",
    "The model architecture of 3D-EPN is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/3depn.png\" alt=\"3D-EPN Architecture\" style=\"width: 800px;\"/>\n",
    "\n",
    "For this exercise, we simplify the model by omitting the classification part - this will not have a big impact since most of the shape completion performance comes from the 3D encoder-decoder unet.\n",
    "\n",
    "The model consists of three parts: The encoder, the bottleneck, and the decoder. Encoder and decoder are constructed with the same architecture, just mirrored.\n",
    "\n",
    "The details of each part are:\n",
    "- **Encoder**: 4 layers, each one containing a 3D convolution (with kernel size 4, as seen in the visualization), a 3D batch norm (except the very first layer), and a leaky ReLU with a negative slope of 0.2. Our goal is to reduce the spatial dimension from 32x32x32 to 1x1x1 and to get the feature dimension from 2 (absolute values and sign) to `num_features * 8`. We do this by using a stride of 2 and padding of 1 for all convolutions except for the last one where we use a stride of 1 and no padding. The feature channels are increased from 2 to `num_features` in the first layer and then doubled with every subsequent layer.\n",
    "- **Decoder**: Same architecture as encoder, just mirrored: Going from `num_features * 8 * 2` (the 2 will be explained later) to 1 (the DF values). The spatial dimensions go from 1x1x1 to 32x32x32. Each layer use a 3D Transpose convolution now, together with 3D batch norm and ReLU (no leaky ReLUs anymore). Note that the last layer uses neither Batch Norms nor a ReLU since we do not want to constrain the range of possible values for the prediction.\n",
    "- **Bottleneck**: This is realized with 2 fully connected layers, each one going from a vector of size 640 (which is `num_features * 8`) to a vector of size 640. Each such layer is followed by a ReLU activation.\n",
    "\n",
    "Some minor details:\n",
    "- **Skip connections** allow the decoder to use information from the encoder and also improve gradient flow. We use it here to connect the output of encoder layer 1 to decoder layer 4, the output of encoder layer 2 to decoder layer 3, and so on. This means that the input to a decoder layer is the concatenation of the previous decoder output with the corresponding encoder output, along the feature dimension. Hence, the number of input features for each decoder layer are twice those of the encoder layers, as mentioned above.\n",
    "- **Log scaling**: You also need to scale the final outputs of the network logarithmically: `out = log(out + 1)`. This is the same transformation you applied to the target shapes in the dataloader before and ensures that prediction and target volumes are comparable.\n",
    "\n",
    "With this in mind, implement the network architecture and `forward()` function in `exercise_3/model/threedepn.py`. You can check your architecture with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type            | Params  \n",
      "----------------------------------------------------\n",
      "0  | conv1        | Conv3d          | 10320   \n",
      "1  | conv2        | Conv3d          | 819360  \n",
      "2  | conv3        | Conv3d          | 3277120 \n",
      "3  | conv4        | Conv3d          | 13107840\n",
      "4  | bn1          | BatchNorm3d     | 320     \n",
      "5  | bn2          | BatchNorm3d     | 640     \n",
      "6  | bn3          | BatchNorm3d     | 1280    \n",
      "7  | leaky_relu   | LeakyReLU       | 0       \n",
      "8  | bottleneck   | Sequential      | 820480  \n",
      "9  | bottleneck.0 | Linear          | 410240  \n",
      "10 | bottleneck.1 | ReLU            | 0       \n",
      "11 | bottleneck.2 | Linear          | 410240  \n",
      "12 | bottleneck.3 | ReLU            | 0       \n",
      "13 | tconv1       | ConvTranspose3d | 26214720\n",
      "14 | tconv2       | ConvTranspose3d | 6553760 \n",
      "15 | tconv3       | ConvTranspose3d | 1638480 \n",
      "16 | tconv4       | ConvTranspose3d | 10241   \n",
      "17 | bn4          | BatchNorm3d     | 640     \n",
      "18 | bn5          | BatchNorm3d     | 320     \n",
      "19 | bn6          | BatchNorm3d     | 160     \n",
      "20 | relu         | ReLU            | 0       \n",
      "21 | TOTAL        | ThreeDEPN       | 52455681\n",
      "Output tensor shape:  torch.Size([4, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.model.threedepn import ThreeDEPN\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPN()\n",
    "print(summarize_model(threedepn))  # Expected: Rows 0-34 and TOTAL = 52455681\n",
    "\n",
    "sdf = torch.randn(4, 1, 32, 32, 32) * 2. - 1.\n",
    "input_tensor = torch.cat([torch.abs(sdf), torch.sign(sdf)], dim=1)\n",
    "predictions = threedepn(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: torch.Size([4, 32, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape reconstruction\n",
    "\n",
    "You can now go to the train script in `exercise_3/training/train_3depn.py` and fill in the missing pieces as you did for exercise 2. Then, verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[004/00001] train_loss: 0.070432\n",
      "[009/00001] train_loss: 0.022295\n",
      "[012/00000] val_loss: 0.527366 | best_loss_val: 0.527366\n",
      "[014/00001] train_loss: 0.013221\n",
      "[019/00001] train_loss: 0.008615\n",
      "[024/00001] train_loss: 0.006580\n",
      "[024/00001] val_loss: 0.203785 | best_loss_val: 0.203785\n",
      "[029/00001] train_loss: 0.005581\n",
      "[034/00001] train_loss: 0.004817\n",
      "[037/00000] val_loss: 0.159729 | best_loss_val: 0.159729\n",
      "[039/00001] train_loss: 0.004245\n",
      "[044/00001] train_loss: 0.003877\n",
      "[049/00001] train_loss: 0.003666\n",
      "[049/00001] val_loss: 0.142577 | best_loss_val: 0.142577\n",
      "[054/00001] train_loss: 0.003449\n",
      "[059/00001] train_loss: 0.003286\n",
      "[062/00000] val_loss: 0.131892 | best_loss_val: 0.131892\n",
      "[064/00001] train_loss: 0.003148\n",
      "[069/00001] train_loss: 0.003061\n",
      "[074/00001] train_loss: 0.002986\n",
      "[074/00001] val_loss: 0.126460 | best_loss_val: 0.126460\n",
      "[079/00001] train_loss: 0.002917\n",
      "[084/00001] train_loss: 0.002839\n",
      "[087/00000] val_loss: 0.122761 | best_loss_val: 0.122761\n",
      "[089/00001] train_loss: 0.002806\n",
      "[094/00001] train_loss: 0.002769\n",
      "[099/00001] train_loss: 0.002710\n",
      "[099/00001] val_loss: 0.120128 | best_loss_val: 0.120128\n",
      "[104/00001] train_loss: 0.002670\n",
      "[109/00001] train_loss: 0.002666\n",
      "[112/00000] val_loss: 0.118830 | best_loss_val: 0.118830\n",
      "[114/00001] train_loss: 0.002632\n",
      "[119/00001] train_loss: 0.002609\n",
      "[124/00001] train_loss: 0.002608\n",
      "[124/00001] val_loss: 0.117721 | best_loss_val: 0.117721\n",
      "[129/00001] train_loss: 0.002606\n",
      "[134/00001] train_loss: 0.002580\n",
      "[137/00000] val_loss: 0.117003 | best_loss_val: 0.117003\n",
      "[139/00001] train_loss: 0.002564\n",
      "[144/00001] train_loss: 0.002582\n",
      "[149/00001] train_loss: 0.002551\n",
      "[149/00001] val_loss: 0.116566 | best_loss_val: 0.116566\n",
      "[154/00001] train_loss: 0.002553\n",
      "[159/00001] train_loss: 0.002548\n",
      "[162/00000] val_loss: 0.116209 | best_loss_val: 0.116209\n",
      "[164/00001] train_loss: 0.002544\n",
      "[169/00001] train_loss: 0.002539\n",
      "[174/00001] train_loss: 0.002531\n",
      "[174/00001] val_loss: 0.116019 | best_loss_val: 0.116019\n",
      "[179/00001] train_loss: 0.002540\n",
      "[184/00001] train_loss: 0.002517\n",
      "[187/00000] val_loss: 0.115867 | best_loss_val: 0.115867\n",
      "[189/00001] train_loss: 0.002528\n",
      "[194/00001] train_loss: 0.002519\n",
      "[199/00001] train_loss: 0.002515\n",
      "[199/00001] val_loss: 0.115774 | best_loss_val: 0.115774\n",
      "[204/00001] train_loss: 0.002517\n",
      "[209/00001] train_loss: 0.002519\n",
      "[212/00000] val_loss: 0.115750 | best_loss_val: 0.115750\n",
      "[214/00001] train_loss: 0.002528\n",
      "[219/00001] train_loss: 0.002513\n",
      "[224/00001] train_loss: 0.002514\n",
      "[224/00001] val_loss: 0.115674 | best_loss_val: 0.115674\n",
      "[229/00001] train_loss: 0.002515\n",
      "[234/00001] train_loss: 0.002510\n",
      "[237/00000] val_loss: 0.115655 | best_loss_val: 0.115655\n",
      "[239/00001] train_loss: 0.002515\n",
      "[244/00001] train_loss: 0.002527\n",
      "[249/00001] train_loss: 0.002507\n",
      "[249/00001] val_loss: 0.115623 | best_loss_val: 0.115623\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.training import train_3depn\n",
    "config = {\n",
    "    'experiment_name': '3_1_3depn_overfitting',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 250,\n",
    "    'print_every_n': 10,\n",
    "    'validate_every_n': 25,\n",
    "}\n",
    "#train_3depn.main(config)  # should be able to get <0.0025 train_loss and <0.13 val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over the entire training set\n",
    "If the overfitting works, we can go ahead with training on the entire dataset.\n",
    "\n",
    "**Note**: As is the case with most reconstruction networks and considering the size of the model (> 50M parameters), this training will take a few hours on a GPU. *Please make sure to start training early enough before the submission deadline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.033006\n",
      "[000/00099] train_loss: 0.014199\n",
      "[000/00149] train_loss: 0.011504\n",
      "[000/00199] train_loss: 0.010343\n",
      "[000/00249] train_loss: 0.009146\n",
      "[000/00299] train_loss: 0.008594\n",
      "[000/00349] train_loss: 0.008901\n",
      "[000/00399] train_loss: 0.008294\n",
      "[000/00449] train_loss: 0.008185\n",
      "[000/00499] train_loss: 0.007624\n",
      "[000/00549] train_loss: 0.007329\n",
      "[000/00599] train_loss: 0.007106\n",
      "[000/00649] train_loss: 0.007092\n",
      "[000/00699] train_loss: 0.007133\n",
      "[000/00749] train_loss: 0.006871\n",
      "[000/00799] train_loss: 0.006601\n",
      "[000/00849] train_loss: 0.006673\n",
      "[000/00899] train_loss: 0.006765\n",
      "[000/00949] train_loss: 0.006230\n",
      "[000/00999] train_loss: 0.006361\n",
      "[000/00999] val_loss: 0.365919 | best_loss_val: 0.365919\n",
      "[000/01049] train_loss: 0.006302\n",
      "[000/01099] train_loss: 0.005775\n",
      "[000/01149] train_loss: 0.006159\n",
      "[000/01199] train_loss: 0.006223\n",
      "[000/01249] train_loss: 0.006039\n",
      "[000/01299] train_loss: 0.005752\n",
      "[000/01349] train_loss: 0.005842\n",
      "[000/01399] train_loss: 0.005596\n",
      "[000/01449] train_loss: 0.005498\n",
      "[000/01499] train_loss: 0.005998\n",
      "[000/01549] train_loss: 0.005745\n",
      "[000/01599] train_loss: 0.005687\n",
      "[000/01649] train_loss: 0.005744\n",
      "[000/01699] train_loss: 0.005316\n",
      "[000/01749] train_loss: 0.005929\n",
      "[000/01799] train_loss: 0.005592\n",
      "[000/01849] train_loss: 0.005460\n",
      "[000/01899] train_loss: 0.005267\n",
      "[000/01949] train_loss: 0.005157\n",
      "[000/01999] train_loss: 0.005405\n",
      "[000/01999] val_loss: 0.145307 | best_loss_val: 0.145307\n",
      "[000/02049] train_loss: 0.005345\n",
      "[000/02099] train_loss: 0.005629\n",
      "[000/02149] train_loss: 0.005353\n",
      "[000/02199] train_loss: 0.005485\n",
      "[000/02249] train_loss: 0.005500\n",
      "[000/02299] train_loss: 0.005139\n",
      "[000/02349] train_loss: 0.005421\n",
      "[000/02399] train_loss: 0.005363\n",
      "[000/02449] train_loss: 0.005016\n",
      "[000/02499] train_loss: 0.005185\n",
      "[000/02549] train_loss: 0.005014\n",
      "[000/02599] train_loss: 0.005379\n",
      "[000/02649] train_loss: 0.005396\n",
      "[000/02699] train_loss: 0.005208\n",
      "[000/02749] train_loss: 0.005079\n",
      "[000/02799] train_loss: 0.005240\n",
      "[000/02849] train_loss: 0.005074\n",
      "[000/02899] train_loss: 0.005073\n",
      "[000/02949] train_loss: 0.004896\n",
      "[000/02999] train_loss: 0.005316\n",
      "[000/02999] val_loss: 0.211973 | best_loss_val: 0.145307\n",
      "[000/03049] train_loss: 0.004495\n",
      "[000/03099] train_loss: 0.004963\n",
      "[000/03149] train_loss: 0.004768\n",
      "[000/03199] train_loss: 0.005101\n",
      "[000/03249] train_loss: 0.004933\n",
      "[000/03299] train_loss: 0.005073\n",
      "[000/03349] train_loss: 0.004701\n",
      "[000/03399] train_loss: 0.004718\n",
      "[000/03449] train_loss: 0.004748\n",
      "[000/03499] train_loss: 0.005113\n",
      "[000/03549] train_loss: 0.004656\n",
      "[000/03599] train_loss: 0.004434\n",
      "[000/03649] train_loss: 0.004505\n",
      "[000/03699] train_loss: 0.004569\n",
      "[000/03749] train_loss: 0.004628\n",
      "[000/03799] train_loss: 0.004694\n",
      "[000/03849] train_loss: 0.004804\n",
      "[000/03899] train_loss: 0.004450\n",
      "[000/03949] train_loss: 0.004764\n",
      "[000/03999] train_loss: 0.004645\n",
      "[000/03999] val_loss: 0.151839 | best_loss_val: 0.145307\n",
      "[000/04049] train_loss: 0.004598\n",
      "[000/04099] train_loss: 0.004531\n",
      "[000/04149] train_loss: 0.004523\n",
      "[000/04199] train_loss: 0.004328\n",
      "[000/04249] train_loss: 0.004699\n",
      "[000/04299] train_loss: 0.004394\n",
      "[000/04349] train_loss: 0.005074\n",
      "[000/04399] train_loss: 0.004719\n",
      "[000/04449] train_loss: 0.004579\n",
      "[000/04499] train_loss: 0.004230\n",
      "[000/04549] train_loss: 0.004437\n",
      "[000/04599] train_loss: 0.004543\n",
      "[000/04649] train_loss: 0.004699\n",
      "[000/04699] train_loss: 0.004758\n",
      "[000/04749] train_loss: 0.004470\n",
      "[001/00000] train_loss: 0.004330\n",
      "[001/00050] train_loss: 0.004467\n",
      "[001/00100] train_loss: 0.004456\n",
      "[001/00150] train_loss: 0.004042\n",
      "[001/00200] train_loss: 0.004299\n",
      "[001/00200] val_loss: 0.228449 | best_loss_val: 0.145307\n",
      "[001/00250] train_loss: 0.004143\n",
      "[001/00300] train_loss: 0.004172\n",
      "[001/00350] train_loss: 0.004050\n",
      "[001/00400] train_loss: 0.004144\n",
      "[001/00450] train_loss: 0.004419\n",
      "[001/00500] train_loss: 0.004045\n",
      "[001/00550] train_loss: 0.004382\n",
      "[001/00600] train_loss: 0.004402\n",
      "[001/00650] train_loss: 0.004032\n",
      "[001/00700] train_loss: 0.004301\n",
      "[001/00750] train_loss: 0.004383\n",
      "[001/00800] train_loss: 0.004411\n",
      "[001/00850] train_loss: 0.004430\n",
      "[001/00900] train_loss: 0.004168\n",
      "[001/00950] train_loss: 0.004083\n",
      "[001/01000] train_loss: 0.004131\n",
      "[001/01050] train_loss: 0.003942\n",
      "[001/01100] train_loss: 0.004186\n",
      "[001/01150] train_loss: 0.004431\n",
      "[001/01200] train_loss: 0.004154\n",
      "[001/01200] val_loss: 0.120674 | best_loss_val: 0.120674\n",
      "[001/01250] train_loss: 0.004173\n",
      "[001/01300] train_loss: 0.004328\n",
      "[001/01350] train_loss: 0.004223\n",
      "[001/01400] train_loss: 0.003853\n",
      "[001/01450] train_loss: 0.004166\n",
      "[001/01500] train_loss: 0.004081\n",
      "[001/01550] train_loss: 0.004143\n",
      "[001/01600] train_loss: 0.004145\n",
      "[001/01650] train_loss: 0.003869\n",
      "[001/01700] train_loss: 0.003938\n",
      "[001/01750] train_loss: 0.003923\n",
      "[001/01800] train_loss: 0.004022\n",
      "[001/01850] train_loss: 0.003915\n",
      "[001/01900] train_loss: 0.004112\n",
      "[001/01950] train_loss: 0.004076\n",
      "[001/02000] train_loss: 0.004295\n",
      "[001/02050] train_loss: 0.004127\n",
      "[001/02100] train_loss: 0.003990\n",
      "[001/02150] train_loss: 0.004050\n",
      "[001/02200] train_loss: 0.004034\n",
      "[001/02200] val_loss: 0.490976 | best_loss_val: 0.120674\n",
      "[001/02250] train_loss: 0.003976\n",
      "[001/02300] train_loss: 0.004136\n",
      "[001/02350] train_loss: 0.004039\n",
      "[001/02400] train_loss: 0.004088\n",
      "[001/02450] train_loss: 0.004120\n",
      "[001/02500] train_loss: 0.003863\n",
      "[001/02550] train_loss: 0.003855\n",
      "[001/02600] train_loss: 0.003796\n",
      "[001/02650] train_loss: 0.003845\n",
      "[001/02700] train_loss: 0.004033\n",
      "[001/02750] train_loss: 0.004131\n",
      "[001/02800] train_loss: 0.003769\n",
      "[001/02850] train_loss: 0.003953\n",
      "[001/02900] train_loss: 0.003985\n",
      "[001/02950] train_loss: 0.003744\n",
      "[001/03000] train_loss: 0.003997\n",
      "[001/03050] train_loss: 0.003912\n",
      "[001/03100] train_loss: 0.003960\n",
      "[001/03150] train_loss: 0.003647\n",
      "[001/03200] train_loss: 0.003662\n",
      "[001/03200] val_loss: 0.137618 | best_loss_val: 0.120674\n",
      "[001/03250] train_loss: 0.003838\n",
      "[001/03300] train_loss: 0.004106\n",
      "[001/03350] train_loss: 0.004180\n",
      "[001/03400] train_loss: 0.003842\n",
      "[001/03450] train_loss: 0.004220\n",
      "[001/03500] train_loss: 0.004226\n",
      "[001/03550] train_loss: 0.003828\n",
      "[001/03600] train_loss: 0.003809\n",
      "[001/03650] train_loss: 0.003712\n",
      "[001/03700] train_loss: 0.003893\n",
      "[001/03750] train_loss: 0.004180\n",
      "[001/03800] train_loss: 0.003880\n",
      "[001/03850] train_loss: 0.004031\n",
      "[001/03900] train_loss: 0.003938\n",
      "[001/03950] train_loss: 0.003904\n",
      "[001/04000] train_loss: 0.003951\n",
      "[001/04050] train_loss: 0.003895\n",
      "[001/04100] train_loss: 0.004424\n",
      "[001/04150] train_loss: 0.003838\n",
      "[001/04200] train_loss: 0.003642\n",
      "[001/04200] val_loss: 0.141467 | best_loss_val: 0.120674\n",
      "[001/04250] train_loss: 0.003887\n",
      "[001/04300] train_loss: 0.003753\n",
      "[001/04350] train_loss: 0.003990\n",
      "[001/04400] train_loss: 0.004075\n",
      "[001/04450] train_loss: 0.003695\n",
      "[001/04500] train_loss: 0.003883\n",
      "[001/04550] train_loss: 0.003785\n",
      "[001/04600] train_loss: 0.003755\n",
      "[001/04650] train_loss: 0.003553\n",
      "[001/04700] train_loss: 0.003573\n",
      "[001/04750] train_loss: 0.003561\n",
      "[002/00001] train_loss: 0.003591\n",
      "[002/00051] train_loss: 0.003616\n",
      "[002/00101] train_loss: 0.003568\n",
      "[002/00151] train_loss: 0.003675\n",
      "[002/00201] train_loss: 0.003719\n",
      "[002/00251] train_loss: 0.003535\n",
      "[002/00301] train_loss: 0.003492\n",
      "[002/00351] train_loss: 0.003403\n",
      "[002/00401] train_loss: 0.003682\n",
      "[002/00401] val_loss: 0.112165 | best_loss_val: 0.112165\n",
      "[002/00451] train_loss: 0.003607\n",
      "[002/00501] train_loss: 0.003479\n",
      "[002/00551] train_loss: 0.003512\n",
      "[002/00601] train_loss: 0.003654\n",
      "[002/00651] train_loss: 0.003642\n",
      "[002/00701] train_loss: 0.003459\n",
      "[002/00751] train_loss: 0.003673\n",
      "[002/00801] train_loss: 0.003617\n",
      "[002/00851] train_loss: 0.003610\n",
      "[002/00901] train_loss: 0.003578\n",
      "[002/00951] train_loss: 0.003531\n",
      "[002/01001] train_loss: 0.003423\n",
      "[002/01051] train_loss: 0.003716\n",
      "[002/01101] train_loss: 0.003467\n",
      "[002/01151] train_loss: 0.003446\n",
      "[002/01201] train_loss: 0.003630\n",
      "[002/01251] train_loss: 0.003509\n",
      "[002/01301] train_loss: 0.003277\n",
      "[002/01351] train_loss: 0.003601\n",
      "[002/01401] train_loss: 0.003524\n",
      "[002/01401] val_loss: 0.129513 | best_loss_val: 0.112165\n",
      "[002/01451] train_loss: 0.003699\n",
      "[002/01501] train_loss: 0.003406\n",
      "[002/01551] train_loss: 0.003534\n",
      "[002/01601] train_loss: 0.003399\n",
      "[002/01651] train_loss: 0.003345\n",
      "[002/01701] train_loss: 0.003724\n",
      "[002/01751] train_loss: 0.003666\n",
      "[002/01801] train_loss: 0.003710\n",
      "[002/01851] train_loss: 0.003486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/01901] train_loss: 0.003525\n",
      "[002/01951] train_loss: 0.003596\n",
      "[002/02001] train_loss: 0.003570\n",
      "[002/02051] train_loss: 0.003497\n",
      "[002/02101] train_loss: 0.003525\n",
      "[002/02151] train_loss: 0.003466\n",
      "[002/02201] train_loss: 0.003572\n",
      "[002/02251] train_loss: 0.003199\n",
      "[002/02301] train_loss: 0.003400\n",
      "[002/02351] train_loss: 0.003530\n",
      "[002/02401] train_loss: 0.003337\n",
      "[002/02401] val_loss: 0.215063 | best_loss_val: 0.112165\n",
      "[002/02451] train_loss: 0.003497\n",
      "[002/02501] train_loss: 0.003403\n",
      "[002/02551] train_loss: 0.003382\n",
      "[002/02601] train_loss: 0.003385\n",
      "[002/02651] train_loss: 0.003300\n",
      "[002/02701] train_loss: 0.003205\n",
      "[002/02751] train_loss: 0.003369\n",
      "[002/02801] train_loss: 0.003430\n",
      "[002/02851] train_loss: 0.003423\n",
      "[002/02901] train_loss: 0.003450\n",
      "[002/02951] train_loss: 0.003288\n",
      "[002/03001] train_loss: 0.003474\n",
      "[002/03051] train_loss: 0.003316\n",
      "[002/03101] train_loss: 0.003530\n",
      "[002/03151] train_loss: 0.003473\n",
      "[002/03201] train_loss: 0.003279\n",
      "[002/03251] train_loss: 0.003571\n",
      "[002/03301] train_loss: 0.003440\n",
      "[002/03351] train_loss: 0.003483\n",
      "[002/03401] train_loss: 0.003358\n",
      "[002/03401] val_loss: 0.249064 | best_loss_val: 0.112165\n",
      "[002/03451] train_loss: 0.003231\n",
      "[002/03501] train_loss: 0.003502\n",
      "[002/03551] train_loss: 0.003293\n",
      "[002/03601] train_loss: 0.003454\n",
      "[002/03651] train_loss: 0.003386\n",
      "[002/03701] train_loss: 0.003338\n",
      "[002/03751] train_loss: 0.003321\n",
      "[002/03801] train_loss: 0.003377\n",
      "[002/03851] train_loss: 0.003250\n",
      "[002/03901] train_loss: 0.003374\n",
      "[002/03951] train_loss: 0.003147\n",
      "[002/04001] train_loss: 0.003255\n",
      "[002/04051] train_loss: 0.003205\n",
      "[002/04101] train_loss: 0.003135\n",
      "[002/04151] train_loss: 0.003359\n",
      "[002/04201] train_loss: 0.003369\n",
      "[002/04251] train_loss: 0.003440\n",
      "[002/04301] train_loss: 0.003127\n",
      "[002/04351] train_loss: 0.003357\n",
      "[002/04401] train_loss: 0.003387\n",
      "[002/04401] val_loss: 0.107891 | best_loss_val: 0.107891\n",
      "[002/04451] train_loss: 0.003458\n",
      "[002/04501] train_loss: 0.003430\n",
      "[002/04551] train_loss: 0.003252\n",
      "[002/04601] train_loss: 0.003412\n",
      "[002/04651] train_loss: 0.003369\n",
      "[002/04701] train_loss: 0.003416\n",
      "[002/04751] train_loss: 0.003518\n",
      "[003/00002] train_loss: 0.003296\n",
      "[003/00052] train_loss: 0.003239\n",
      "[003/00102] train_loss: 0.003106\n",
      "[003/00152] train_loss: 0.003132\n",
      "[003/00202] train_loss: 0.003177\n",
      "[003/00252] train_loss: 0.003131\n",
      "[003/00302] train_loss: 0.003067\n",
      "[003/00352] train_loss: 0.003209\n",
      "[003/00402] train_loss: 0.003227\n",
      "[003/00452] train_loss: 0.003202\n",
      "[003/00502] train_loss: 0.003160\n",
      "[003/00552] train_loss: 0.003333\n",
      "[003/00602] train_loss: 0.003329\n",
      "[003/00602] val_loss: 0.787819 | best_loss_val: 0.107891\n",
      "[003/00652] train_loss: 0.003016\n",
      "[003/00702] train_loss: 0.003077\n",
      "[003/00752] train_loss: 0.003223\n",
      "[003/00802] train_loss: 0.003129\n",
      "[003/00852] train_loss: 0.003016\n",
      "[003/00902] train_loss: 0.003086\n",
      "[003/00952] train_loss: 0.003120\n",
      "[003/01002] train_loss: 0.003163\n",
      "[003/01052] train_loss: 0.003236\n",
      "[003/01102] train_loss: 0.003121\n",
      "[003/01152] train_loss: 0.003163\n",
      "[003/01202] train_loss: 0.002942\n",
      "[003/01252] train_loss: 0.002977\n",
      "[003/01302] train_loss: 0.003032\n",
      "[003/01352] train_loss: 0.002986\n",
      "[003/01402] train_loss: 0.003220\n",
      "[003/01452] train_loss: 0.003175\n",
      "[003/01502] train_loss: 0.003196\n",
      "[003/01552] train_loss: 0.003032\n",
      "[003/01602] train_loss: 0.003245\n",
      "[003/01602] val_loss: 0.103948 | best_loss_val: 0.103948\n",
      "[003/01652] train_loss: 0.003120\n",
      "[003/01702] train_loss: 0.003001\n",
      "[003/01752] train_loss: 0.003018\n",
      "[003/01802] train_loss: 0.003207\n",
      "[003/01852] train_loss: 0.003223\n",
      "[003/01902] train_loss: 0.003017\n",
      "[003/01952] train_loss: 0.003070\n",
      "[003/02002] train_loss: 0.003111\n",
      "[003/02052] train_loss: 0.002989\n",
      "[003/02102] train_loss: 0.003118\n",
      "[003/02152] train_loss: 0.003032\n",
      "[003/02202] train_loss: 0.003031\n",
      "[003/02252] train_loss: 0.002929\n",
      "[003/02302] train_loss: 0.002904\n",
      "[003/02352] train_loss: 0.002890\n",
      "[003/02402] train_loss: 0.003048\n",
      "[003/02452] train_loss: 0.003093\n",
      "[003/02502] train_loss: 0.003013\n",
      "[003/02552] train_loss: 0.003160\n",
      "[003/02602] train_loss: 0.003048\n",
      "[003/02602] val_loss: 0.128195 | best_loss_val: 0.103948\n",
      "[003/02652] train_loss: 0.003177\n",
      "[003/02702] train_loss: 0.003429\n",
      "[003/02752] train_loss: 0.003145\n",
      "[003/02802] train_loss: 0.002970\n",
      "[003/02852] train_loss: 0.002988\n",
      "[003/02902] train_loss: 0.002961\n",
      "[003/02952] train_loss: 0.003168\n",
      "[003/03002] train_loss: 0.003189\n",
      "[003/03052] train_loss: 0.002983\n",
      "[003/03102] train_loss: 0.003153\n",
      "[003/03152] train_loss: 0.002931\n",
      "[003/03202] train_loss: 0.002973\n",
      "[003/03252] train_loss: 0.002920\n",
      "[003/03302] train_loss: 0.003128\n",
      "[003/03352] train_loss: 0.002989\n",
      "[003/03402] train_loss: 0.003067\n",
      "[003/03452] train_loss: 0.003034\n",
      "[003/03502] train_loss: 0.002929\n",
      "[003/03552] train_loss: 0.003189\n",
      "[003/03602] train_loss: 0.003046\n",
      "[003/03602] val_loss: 0.123101 | best_loss_val: 0.103948\n",
      "[003/03652] train_loss: 0.003266\n",
      "[003/03702] train_loss: 0.003064\n",
      "[003/03752] train_loss: 0.002934\n",
      "[003/03802] train_loss: 0.002957\n",
      "[003/03852] train_loss: 0.003147\n",
      "[003/03902] train_loss: 0.003187\n",
      "[003/03952] train_loss: 0.002853\n",
      "[003/04002] train_loss: 0.002945\n",
      "[003/04052] train_loss: 0.002856\n",
      "[003/04102] train_loss: 0.002979\n",
      "[003/04152] train_loss: 0.002877\n",
      "[003/04202] train_loss: 0.002993\n",
      "[003/04252] train_loss: 0.003011\n",
      "[003/04302] train_loss: 0.002992\n",
      "[003/04352] train_loss: 0.003020\n",
      "[003/04402] train_loss: 0.003093\n",
      "[003/04452] train_loss: 0.002886\n",
      "[003/04502] train_loss: 0.003057\n",
      "[003/04552] train_loss: 0.002905\n",
      "[003/04602] train_loss: 0.002913\n",
      "[003/04602] val_loss: 0.097932 | best_loss_val: 0.097932\n",
      "[003/04652] train_loss: 0.002896\n",
      "[003/04702] train_loss: 0.003121\n",
      "[003/04752] train_loss: 0.003072\n",
      "[004/00003] train_loss: 0.002991\n",
      "[004/00053] train_loss: 0.002771\n",
      "[004/00103] train_loss: 0.002862\n",
      "[004/00153] train_loss: 0.002691\n",
      "[004/00203] train_loss: 0.002620\n",
      "[004/00253] train_loss: 0.002755\n",
      "[004/00303] train_loss: 0.002673\n",
      "[004/00353] train_loss: 0.002789\n",
      "[004/00403] train_loss: 0.002763\n",
      "[004/00453] train_loss: 0.002771\n",
      "[004/00503] train_loss: 0.002760\n",
      "[004/00553] train_loss: 0.002628\n",
      "[004/00603] train_loss: 0.002723\n",
      "[004/00653] train_loss: 0.002786\n",
      "[004/00703] train_loss: 0.002781\n",
      "[004/00753] train_loss: 0.002964\n",
      "[004/00803] train_loss: 0.002855\n",
      "[004/00803] val_loss: 0.197200 | best_loss_val: 0.097932\n",
      "[004/00853] train_loss: 0.002787\n",
      "[004/00903] train_loss: 0.003077\n",
      "[004/00953] train_loss: 0.002839\n",
      "[004/01003] train_loss: 0.002772\n",
      "[004/01053] train_loss: 0.002817\n",
      "[004/01103] train_loss: 0.002755\n",
      "[004/01153] train_loss: 0.002661\n",
      "[004/01203] train_loss: 0.002597\n",
      "[004/01253] train_loss: 0.002674\n",
      "[004/01303] train_loss: 0.002704\n",
      "[004/01353] train_loss: 0.002751\n",
      "[004/01403] train_loss: 0.002755\n",
      "[004/01453] train_loss: 0.002784\n",
      "[004/01503] train_loss: 0.002635\n",
      "[004/01553] train_loss: 0.002797\n",
      "[004/01603] train_loss: 0.002878\n",
      "[004/01653] train_loss: 0.003078\n",
      "[004/01703] train_loss: 0.002770\n",
      "[004/01753] train_loss: 0.002857\n",
      "[004/01803] train_loss: 0.002701\n",
      "[004/01803] val_loss: 0.114974 | best_loss_val: 0.097932\n",
      "[004/01853] train_loss: 0.002841\n",
      "[004/01903] train_loss: 0.002849\n",
      "[004/01953] train_loss: 0.002717\n",
      "[004/02003] train_loss: 0.002697\n",
      "[004/02053] train_loss: 0.002823\n",
      "[004/02103] train_loss: 0.002729\n",
      "[004/02153] train_loss: 0.002639\n",
      "[004/02203] train_loss: 0.002818\n",
      "[004/02253] train_loss: 0.002833\n",
      "[004/02303] train_loss: 0.002596\n",
      "[004/02353] train_loss: 0.002751\n",
      "[004/02403] train_loss: 0.002953\n",
      "[004/02453] train_loss: 0.002709\n",
      "[004/02503] train_loss: 0.002747\n",
      "[004/02553] train_loss: 0.002915\n",
      "[004/02603] train_loss: 0.002893\n",
      "[004/02653] train_loss: 0.002845\n",
      "[004/02703] train_loss: 0.002936\n",
      "[004/02753] train_loss: 0.002750\n",
      "[004/02803] train_loss: 0.002815\n",
      "[004/02803] val_loss: 0.183824 | best_loss_val: 0.097932\n",
      "[004/02853] train_loss: 0.002871\n",
      "[004/02903] train_loss: 0.002691\n",
      "[004/02953] train_loss: 0.002733\n",
      "[004/03003] train_loss: 0.002932\n",
      "[004/03053] train_loss: 0.002741\n",
      "[004/03103] train_loss: 0.002750\n",
      "[004/03153] train_loss: 0.002758\n",
      "[004/03203] train_loss: 0.002912\n",
      "[004/03253] train_loss: 0.002621\n",
      "[004/03303] train_loss: 0.002783\n",
      "[004/03353] train_loss: 0.002914\n",
      "[004/03403] train_loss: 0.002703\n",
      "[004/03453] train_loss: 0.002680\n",
      "[004/03503] train_loss: 0.002849\n",
      "[004/03553] train_loss: 0.002782\n",
      "[004/03603] train_loss: 0.002700\n",
      "[004/03653] train_loss: 0.002865\n",
      "[004/03703] train_loss: 0.002880\n",
      "[004/03753] train_loss: 0.002802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004/03803] train_loss: 0.002726\n",
      "[004/03803] val_loss: 0.123703 | best_loss_val: 0.097932\n",
      "[004/03853] train_loss: 0.002607\n",
      "[004/03903] train_loss: 0.002880\n",
      "[004/03953] train_loss: 0.002847\n",
      "[004/04003] train_loss: 0.002806\n",
      "[004/04053] train_loss: 0.002759\n",
      "[004/04103] train_loss: 0.002992\n",
      "[004/04153] train_loss: 0.002830\n",
      "[004/04203] train_loss: 0.002726\n",
      "[004/04253] train_loss: 0.002940\n",
      "[004/04303] train_loss: 0.002944\n",
      "[004/04353] train_loss: 0.002801\n",
      "[004/04403] train_loss: 0.002629\n",
      "[004/04453] train_loss: 0.002714\n",
      "[004/04503] train_loss: 0.002674\n",
      "[004/04553] train_loss: 0.002633\n",
      "[004/04603] train_loss: 0.002784\n",
      "[004/04653] train_loss: 0.002696\n",
      "[004/04703] train_loss: 0.002609\n",
      "[004/04753] train_loss: 0.002687\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'experiment_name': '3_1_3depn_generalization',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 5,\n",
    "    'print_every_n': 50,\n",
    "    'validate_every_n': 1000,\n",
    "}\n",
    "#train_3depn.main(config)  # should be able to get best_loss_val < 0.1 after a few hours and 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference\n",
    "\n",
    "Implement the missing bits in `exercise_3/inference/infer_3depn.py`. You should then be able to see your reconstructions below.\n",
    "\n",
    "The outputs of our provided visualization functions are, from left to right:\n",
    "- Input, partial shape\n",
    "- Predicted completion\n",
    "- Target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3.util.visualization import visualize_meshes\n",
    "from exercise_3.inference.infer_3depn import InferenceHandler3DEPN\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandler3DEPN('exercise_3/runs/3_1_3depn_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1385d5ef5e14ea88db31e8ef6131f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/b286c9c136784db2af1744fdb1fbe7df__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3125009b5e472a9cb9f1f5ac9f27ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__1__')\n",
    "target_df = ShapeNet.get_shape_df('03636649/23eaba9bdd51a5b0dfe9cab879fd37e8__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94310a01a054767977ea9d615c33a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_sdf = ShapeNet.get_shape_sdf('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "target_df = ShapeNet.get_shape_df('02691156/5de2cc606b65b960e0b6546e08902f28__0__')\n",
    "\n",
    "input_mesh, reconstructed_mesh, target_mesh = inferer.infer_single(input_sdf, target_df)\n",
    "visualize_meshes([input_mesh, reconstructed_mesh, target_mesh], flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DeepSDF\n",
    "\n",
    "\n",
    "Here, we will take a look at 3D-reconstruction using [DeepSDF](https://arxiv.org/abs/1901.05103). We recommend reading the paper before attempting the exercise.\n",
    "\n",
    "DeepSDF is an auto-decoder based approach that learns a continuous SDF representation for a class of shapes. Once trained, it can be used for shape representation, interpolation and shape completion. We'll look at each of these\n",
    "applications.\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_teaser.png\" alt=\"deepsdf_teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "During training, the autodecoder optimizes both the network parameters and the latent codes representing each of the training shapes. Once trained, to reconstruct a shape given its SDF observations, a latent code is\n",
    "optimized keeping the network parameters fixed, such that the optimized latent code gives the lowest error with observed SDF values.\n",
    "\n",
    "An advantage that implicit representations have over voxel/grid based approaches is that they are not tied to a particular grid resolution, and can be evaluated at any resolution once trained.\n",
    "\n",
    "Similar to previous exercise, we'll first download the processed dataset, look at the implementation of the dataset, the model and the trainer, try out overfitting and generalization over the entire dataset, and finally inference on unseen samples.\n",
    "\n",
    "### (a) Downloading the data\n",
    "\n",
    "Whereas volumetric models output entire 3d shape representations, implicit models like DeepSDF work on per point basis. The network takes in a 3D-coordinate (and additionally the latent vector) and outputs the SDF value at the queried point. To train such a model,\n",
    "we therefore need, for each of the training shapes, a bunch of points with their corresponding SDF values for supervision. Points are sampled more aggressively near the surface of the object as we want to capture a more detailed SDF near the surface. For those curious,\n",
    "data preparation is decribed in more detail in section 5 of the paper.\n",
    "\n",
    "We'll be using the ShapeNet Sofa class for the experiments in this exercise. We've already prepared this data, so that you don't need to deal with the preprocessing. For each shape, the following files are provided:\n",
    "- `mesh.obj` representing the mesh representation of the shape\n",
    "- `sdf.npz` file containing large number of points sampled on and around the mesh and their sdf values; contains numpy arrays under keys \"pos\" and \"neg\", containing points with positive and negative sdf values respectively\n",
    "\n",
    "```\n",
    "# contents of exercise_3/data/sdf_sofas\n",
    "1faa4c299b93a3e5593ebeeedbff73b/                    # shape 0\n",
    "    ├── mesh.obj                                    # shape 0 mesh\n",
    "    ├── sdf.npz                                     # shape 0 sdf\n",
    "    ├── surface.obj                                 # shape 0 surface\n",
    "1fde48d83065ef5877a929f61fea4d0/                    # shape 1\n",
    "1fe1411b6c8097acf008d8a3590fb522/                   # shape 2\n",
    ":\n",
    "```\n",
    "Download and extract the data with the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "Extracting ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print('Downloading ...')\n",
    "# File sizes: ~10GB\n",
    "!wget https://www.dropbox.com/s/4k5pw126nzus8ef/sdf_sofas.zip\\?dl\\=0 -O exercise_3/data/sdf_sofas.zip -P exercise_3/data\n",
    "\n",
    "print('Extracting ...')\n",
    "!unzip -q /usr/home/sut/datasets/e3/sdf_sofas.zip -d /usr/home/sut/datasets/e3\n",
    "!rm /usr/home/sut/datasets/e3/sdf_sofas.zip\n",
    "\n",
    "print('Done.')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "We provide a partial implementation of the dataset in `exercise_3/data/shape_implicit.py`.\n",
    "Your task is to complete the `#TODOs` so that the dataset works as specified by the docstrings.\n",
    "\n",
    "Once done, you can try running the following code blocks as sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 1226\n",
      "Length of val set: 137\n",
      "Length of overfit set: 1\n"
     ]
    }
   ],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "\n",
    "num_points_to_samples = 40000\n",
    "train_dataset = ShapeImplicit(num_points_to_samples, \"train\")\n",
    "val_dataset = ShapeImplicit(num_points_to_samples, \"val\")\n",
    "overfit_dataset = ShapeImplicit(num_points_to_samples, \"overfit\")\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 1226\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the points sampled for a particular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31222817  0.04896372 -0.10426033  0.08805085]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9f2635780138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexercise_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_mesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize_pointcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshape_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'points'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sdf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/ml3d/E3/exercise_3/data/shape_implicit.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msdf_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sdf_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf_samples_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from exercise_3.util.visualization import visualize_mesh, visualize_pointcloud\n",
    "\n",
    "shape_id = train_dataset[0]['name']\n",
    "points = train_dataset[0]['points']\n",
    "sdf = train_dataset[0]['sdf']\n",
    "\n",
    "# sampled points inside the shape\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "\n",
    "# sampled points outside the shape\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mesh = ShapeImplicit.get_mesh(shape_id)\n",
    "print('Mesh')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Sampled points with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Sampled points with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that more points are sampled close to the surface rather than away from the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (c) Model\n",
    "\n",
    "The DeepSDF auto-decoder architecture is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_architecture.png\" alt=\"deepsdf_arch\" style=\"width: 640px;\"/>\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- The network takes in the latent code for a shape concatenated with the query 3d coordinate, making up a 259 length vector (assuming latent code length is 256).\n",
    "- The network consist of a sequence of weight-normed linear layers, each followed by a ReLU and a dropout. For weight norming a layer, check out `torch.nn.utils.weight_norm`. Each of these linear layers outputs a 512 dimensional vector, except the 4th layer which outputs a 253 dimensional vector.\n",
    "- The output of the 4th layer is concatenated with the input, making the input to the 5th layer a 512 dimensional vector.\n",
    "- The final layer is a simple linear layer without any norm, dropout or non-linearity, with a single dimensional output representing the SDF value.\n",
    "\n",
    "Implement this architecture in file `exercise_3/model/deepsdf.py`.\n",
    "\n",
    "Here are some basic sanity tests once you're done with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.model.deepsdf import DeepSDFDecoder\n",
    "from exercise_3.util.model import summarize_model\n",
    "\n",
    "deepsdf = DeepSDFDecoder(latent_size=256)\n",
    "print(summarize_model(deepsdf))\n",
    "\n",
    "# input to the network is a concatenation of point coordinates (3) and the latent code (256 in this example);\n",
    "# here we use a batch of 4096 points\n",
    "input_tensor = torch.randn(4096, 3 + 256)\n",
    "predictions = deepsdf(input_tensor)\n",
    "\n",
    "print('\\nOutput tensor shape: ', predictions.shape)  # expected output: 4096, 1\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in deepsdf.parameters() if p.requires_grad) / 1e6\n",
    "print(f'\\nNumber of traininable params: {num_trainable_params:.2f}M')  # expected output: ~1.8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape\n",
    "\n",
    "Fill in the train script in `exercise_3/training/train_deepsdf.py`, and verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "overfit_config = {\n",
    "    'experiment_name': '3_2_deepsdf_overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'num_sample_points': 4096,\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 2000,\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 250,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(overfit_config)  # expected loss around 0.0062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the overfitted shape reconstruction to check if it looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize GT mesh of the overfit sample\n",
    "gt_mesh = ShapeImplicit.get_mesh('7e728818848f191bee7d178666aae23d')\n",
    "print('GT')\n",
    "visualize_mesh(gt_mesh.vertices, gt_mesh.faces, flip_axes=True)\n",
    "\n",
    "# Load and visualize reconstructed overfit sample; it's okay if they don't look visually exact, since we don't run \n",
    "# the training too long and have a learning rate decay while training \n",
    "mesh_path = \"exercise_3/runs/3_2_deepsdf_overfit/meshes/01999_000.obj\"\n",
    "overfit_output = trimesh.load(mesh_path)\n",
    "print('Overfit')\n",
    "visualize_mesh(overfit_output.vertices, overfit_output.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over entire train set\n",
    "\n",
    "Once overfitting works, we can train on the entire train set.\n",
    "\n",
    "Note: This training will take a few hours on a GPU (took ~3 hrs for 500 epochs on our 2080Ti, which already gave decent results). Please make sure to start training early enough before the submission deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.training import train_deepsdf\n",
    "\n",
    "generalization_config = {\n",
    "    'experiment_name': '3_2_deepsdf_generalization',\n",
    "    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n",
    "    'is_overfit': False,\n",
    "    'num_sample_points': 4096, # you can adjust this such that the model fits on your gpu\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 2000,  # not necessary to run for 2000 epochs if you're short on time, at 500 epochs you should start to see reasonable results\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 5000,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(generalization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model on observed SDF values\n",
    "\n",
    "Fill in the inference script `exercise_3/inference/infer_deepsdf.py`. Note that it's not simply a forward pass, but an optimization of the latent code such that we have lowest error on observed SDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "device = torch.device('cuda:0')  # change this to cpu if you're not using a gpu\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try inference on a shape from validation set, for which we have a complete observation of sdf values. This is an easier problem as compared to shape completion,\n",
    "since we have all the information already in the input.\n",
    "\n",
    "Let's visualize the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are very complete\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reconstruction on these observations with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try the shape completion task, i.e., inference on a shape from validation set, for which we do not have a complete observation of sdf values. The observed points are visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get observed data\n",
    "points, sdf = ShapeImplicit.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93_incomplete\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are incomplete\n",
    "# making this is a shape completion task\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape completion using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Latent space interpolation\n",
    "\n",
    "The latent space learned by DeepSDF is interpolatable, meaning that decoding latent codes from this space produced meaningful shapes. Given two latent codes, a linearly interpolatable latent space will decode\n",
    "each of the intermediate codes to some valid shape. Let's see if this holds for our trained model.\n",
    "\n",
    "We'll pick two shapes from the train set as visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3.data.shape_implicit import ShapeImplicit\n",
    "from exercise_3.util.visualization import visualize_mesh\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"494fe53da65650b8c358765b76c296\")\n",
    "print('GT Shape A')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)\n",
    "\n",
    "mesh = ShapeImplicit.get_mesh(\"5ca1ef55ff5f68501921e7a85cf9da35\")\n",
    "print('GT Shape B')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the missing parts in `exercise_3/inference/infer_deepsdf.py` such that it interpolates two given latent vectors, and run the code fragement below once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_2_deepsdf_generalization\", torch.device('cuda:0'))\n",
    "# interpolate; also exports interpolated meshes to disk\n",
    "inference_handler.interpolate('494fe53da65650b8c358765b76c296', '5ca1ef55ff5f68501921e7a85cf9da35', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the interpolation below. If everything works out correctly, you should see a smooth transformation between the shapes, with all intermediate shapes being valid sofas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_3.util.mesh_collection_to_gif import  meshes_to_gif\n",
    "from exercise_3.util.misc import show_gif\n",
    "\n",
    "# create list of meshes (just exported) to be visualized\n",
    "mesh_paths = sorted([x for x in Path(\"exercise_3/runs/3_2_deepsdf_generalization/interpolation\").iterdir() if int(x.name.split('.')[0].split(\"_\")[1]) == 0], key=lambda x: int(x.name.split('.')[0].split(\"_\")[0]))\n",
    "mesh_paths = mesh_paths + mesh_paths[::-1]\n",
    "\n",
    "# create a visualization of the interpolation process\n",
    "meshes_to_gif(mesh_paths, \"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\", 20)\n",
    "show_gif(\"exercise_3/runs/3_2_deepsdf_generalization/latent_interp.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submission\n",
    "\n",
    "This is the end of exercise 3 🙂. Please create a zip containing all files we provided, everything you modified, your visualization images/gif (no need to submit generated OBJs), including your checkpoints. Name it with your matriculation number(s) as described in exercise 1. Make sure this notebook can be run without problems. Then, submit via Moodle.\n",
    "\n",
    "**Note**: The maximum submission file size limit for Moodle is 100M. You do not need to submit your overfitting checkpoints; however, the generalization checkpoint will be >200M. The easiest way to still be able to submit that one is to split it with zip like this: `zip -s 100M model_best.ckpt.zip model_best.ckpt` which creates a `.zip` and a `.z01`. You can then submit both files alongside another zip containing all your code and outputs.\n",
    "\n",
    "**Submission Deadline**: 09.06.2021, 23:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Dai, Angela, Charles Ruizhongtai Qi, and Matthias Nießner. \"Shape completion using 3d-encoder-predictor cnns and shape synthesis.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[2] Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
